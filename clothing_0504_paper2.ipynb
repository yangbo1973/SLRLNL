{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OMP_NUM_THREADS set to 24, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.static import InputSpec\n",
    "from paddle.fluid.framework import core\n",
    "from paddle.vision import image_load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "batch_size = 32\n",
    "samples = 260000\n",
    "\n",
    "test_samples = 10526\n",
    "n_class = 14\n",
    "training_epochs = 20\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "n_C = 0.1\n",
    "n_V = 0.01\n",
    "n_R = 0.02\n",
    "t_w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(paddle.io.Dataset):\n",
    "    def __init__(self, num_samples, dir_, save_path, transform):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.dir_ = dir_\n",
    "        self.data_dir = []\n",
    "        self.label_list = []\n",
    "        with open(self.dir_) as f:\n",
    "            for line in f:\n",
    "                temp_dir, label = line.split()\n",
    "                self.data_dir.append(temp_dir)\n",
    "                self.label_list.append(int(label))\n",
    "        \n",
    "        self.labels = np.array([l for l in self.label_list])\n",
    "        np.save(save_path, self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        f_path = self.data_dir[index].split('/')\n",
    "        dir_ = r'/root/clothing1m'\n",
    "        path = os.path.join(dir_,*f_path)\n",
    "        image = image_load(path)\n",
    "#         image = jpeg.JPEG(path).decode()\n",
    "        data = np.array(image)\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "            data = np.tile(data, (1,1,3))\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return index, data\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download training data and load training data.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\n",
    "\n",
    "mean,std = ((0.6959, 0.6537, 0.6371), (0.3113, 0.3192, 0.3214))\n",
    "mean = np.array(mean).reshape(1,1,3)*255\n",
    "std = np.array(std).reshape(1,1,3)*255\n",
    "\n",
    "transform_robust = T.Compose([\n",
    "                    T.RandomHorizontalFlip(),\n",
    "                    T.Resize(256),\n",
    "                    T.RandomCrop(image_size),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "transform_show = T.Compose([\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    ])\n",
    "transform_clean = T.Compose([\n",
    "#                     T.ColorJitter(),\n",
    "                    T.Resize((256, 256)),\n",
    "                    T.CenterCrop((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "print('Start download training data and load training data.')\n",
    "train_dataset = Dataset(samples, 'pseudo_noisy_train_key_list_kv.txt', 'train_Y_noised_pseudo.npy', transform_robust)\n",
    "testclean_dataset = Dataset(test_samples, 'clean_test_key_list_kv.txt', 'testclean_Y.npy', transform_clean)\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 6)\n",
    "test_loader = paddle.io.DataLoader(testclean_dataset, batch_size=128, shuffle=False, num_workers = 6)\n",
    "\n",
    "\n",
    "train_Y_noised = np.load('train_Y_noised_pseudo.npy')[:samples]\n",
    "train_Y = train_Y_noised\n",
    "test_Y = np.load('testclean_Y.npy')[:test_samples]\n",
    "Yt_list = [train_Y_noised]\n",
    "print('Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    loader = test_loader()\n",
    "    loss_eval = 0\n",
    "    acc_eval = 0\n",
    "    network_3.eval()\n",
    "    Py_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Feature_temp = np.zeros((test_samples,network_3.fc.weight.shape[0]),dtype = np.float32)\n",
    "    with paddle.no_grad():\n",
    "        for batch_id, (index,X_data) in enumerate(loader):\n",
    "            Y_data = np.array(test_Y[index]).astype(np.int64)\n",
    "            temp_X = paddle.to_tensor(X_data)\n",
    "            Y_GPU = paddle.to_tensor(Y_data)\n",
    "            y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "            first = network_3.maxpool(network_3.relu(network_3.bn1(network_3.conv1(temp_X))))\n",
    "            l1 = network_3.layer1(first)\n",
    "            l2 = network_3.layer2(l1)\n",
    "            l3 = network_3.layer3(l2)\n",
    "            l4 = network_3.layer4(l3)\n",
    "            feature = paddle.reshape(network_3.avgpool(l4), (l4.shape[0],-1))\n",
    "\n",
    "            logits = network_3.fc(feature)\n",
    "            probs = paddle.nn.functional.softmax(logits)\n",
    "            Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "            Py_temp[index] = Py.numpy()\n",
    "            Feature_temp[index] = feature.numpy()\n",
    "            \n",
    "            loss = loss_fn(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "            \n",
    "            acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "            loss_eval += loss.numpy()\n",
    "            acc_eval += acc.numpy()\n",
    "\n",
    "        loss_eval/=(batch_id+1)\n",
    "        acc_eval/=(batch_id+1)\n",
    "    return loss_eval, acc_eval, Feature_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 09:48:47.635505  9877 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.2\n",
      "W0504 09:48:47.639564  9877 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits_list = []\n",
    "Pred_list = []\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "loss_train_list = []\n",
    "\n",
    "Py_list = [np.random.rand(samples,)]\n",
    "score_list = [np.random.rand(samples,)]\n",
    "\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "network_3 = paddle.vision.models.resnet50(num_classes=n_class,pretrained=True)\n",
    "\n",
    "\n",
    "scheduel = paddle.optimizer.lr.MultiStepDecay(learning_rate=learning_rate, milestones=[3, 6, 9], gamma=0.1)\n",
    "\n",
    "opt = paddle.optimizer.Momentum(learning_rate = scheduel,\n",
    "                                parameters=network_3.parameters(),)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch_id in range(training_epochs):\n",
    "    network_3.train()\n",
    "    feature_num = network_3.fc.parameters()[0].shape[0]\n",
    "    \n",
    "    loader = train_loader()\n",
    "\n",
    "    loss_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp_other = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp_other = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "    Pred_onehot_temp = np.ones((samples,n_class),dtype=np.float32)  \n",
    "    \n",
    "    thres = np.sort(Py_list[-1])[int(samples*n_R)]    \n",
    "    relabel_mask = Py_list[-1] <= thres   \n",
    "\n",
    "    Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "    Y_onehot_0 = np.eye(n_class)[Yt_list[0]]\n",
    "    for batch_id, (index, X_data) in enumerate(loader):\n",
    "        Y_data = np.array(Yt_list[-1][index]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[index]).astype(np.int64)\n",
    "        pred_onehot_temp = Pred_onehot_temp[index]\n",
    "                \n",
    "        temp_X = paddle.to_tensor(X_data)\n",
    "        Y_GPU = paddle.to_tensor(Y_data)\n",
    "        Y_GPU_ori = paddle.to_tensor(Y_data_ori)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "        first = network_3.maxpool(network_3.relu(network_3.bn1(network_3.conv1(temp_X))))\n",
    "        l1 = network_3.layer1(first)\n",
    "        l2 = network_3.layer2(l1)\n",
    "        l3 = network_3.layer3(l2)\n",
    "        l4 = network_3.layer4(l3)\n",
    "        feature = paddle.reshape(network_3.avgpool(l4), (l4.shape[0],-1))\n",
    "\n",
    "        logits = network_3.fc(feature)\n",
    "\n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pred = paddle.argmax(probs,axis=-1)\n",
    "        logits_other = logits - y_onehot*1e10\n",
    "        Pred_other = paddle.argmax(logits_other,axis=-1)\n",
    "\n",
    "        if epoch_id < t_w:\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "        else:\n",
    "            Y_GPU = paddle.where(paddle.to_tensor(relabel_mask[index]), Pred_other, Y_GPU)\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "            \n",
    "        Py_temp[index] = Py.numpy()\n",
    "        Pred_temp[index] = Pred.numpy()\n",
    "\n",
    "        Probs_temp[index] = probs.numpy()\n",
    "        Logits_temp[index] = logits.numpy()\n",
    "        feature_temp[index] = feature.numpy()\n",
    "                \n",
    "        \n",
    "        acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "        acc_ori = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "        loss_ori = loss_fn(logits,paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "\n",
    "        loss_train += loss.numpy()\n",
    "        acc_train += acc.numpy()\n",
    "        acc_train_ori += acc_ori.numpy()\n",
    "        loss_train_ori += loss_ori.numpy()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        if batch_id%int(4000/batch_size)==0:\n",
    "            _acc = np.sum(Pred.numpy().ravel()==Y_data.ravel(),axis=-1)/Y_data.shape[0]  \n",
    "            _loss = loss.numpy()\n",
    "            end_time =time.time()\n",
    "            print('batch_id:%4d, train loss :%.4f, acc:%.4f, time elapsed %4d'%(batch_id, _loss, _acc, end_time - start_time), flush=True)\n",
    "    scheduel.step()\n",
    "        \n",
    "    loss_train/=(batch_id+1)\n",
    "    acc_train/=(batch_id+1)\n",
    "    acc_train_ori/=(batch_id+1)\n",
    "    loss_train_ori/=(batch_id+1)\n",
    "\n",
    "    print('epoch %d train complete'%epoch_id)\n",
    "    loss_eval, acc_eval, feature_val = validation()\n",
    "\n",
    "    loss_train_list.append(loss_train)\n",
    "\n",
    "    loss_list.append(loss_eval)\n",
    "    acc_list.append(acc_eval)\n",
    "    Pred_list.append(Pred_temp)\n",
    "    Py_list.append(Py_temp)\n",
    "\n",
    "\n",
    "    logits_list.append(Logits_temp)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print_str = 'train loss:%.4f, train acc:%.4f, train acc ori:%.4f,  eval loss:%.4f, eval acc:%.4f, time elapsed:%.4f'%(loss_train, acc_train, acc_train_ori, loss_eval, acc_eval, end_time - start_time)\n",
    "    print(print_str)  \n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write(print_str + '\\n')\n",
    "\n",
    "    anchor_points = []\n",
    "    anchor_mask = np.zeros((samples,),dtype=bool)\n",
    "    Py_mean = np.zeros((samples,))\n",
    "\n",
    "    for j in range(len(Py_list)):\n",
    "        Py_mean+=Py_list[j]\n",
    "    Py_mean/=len(Py_list)   \n",
    "\n",
    "    for j in range(n_class):\n",
    "        class_mask = Yt_list[0] == j\n",
    "        c_n = class_mask.sum()\n",
    "        c_th = np.sort(Py_mean[class_mask])[-int(c_n * n_C)]\n",
    "        anchor_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "    anchor_points = np.concatenate(anchor_points)\n",
    "    anchor_mask[anchor_points] = True\n",
    "\n",
    "    L_batch = 1000\n",
    "    Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "    zC = paddle.to_tensor(feature_temp[anchor_points])\n",
    "    fC = paddle.to_tensor(Probs_temp[anchor_points])\n",
    "    yC = paddle.to_tensor(Y_onehot[anchor_points])\n",
    "    fCcyC = fC - yC\n",
    "    lr = 1e-6\n",
    "    learning_risk = np.zeros((samples,)) \n",
    "    for j in range(int(np.ceil(samples/L_batch))):\n",
    "        i_ind = np.arange(j*L_batch, min(samples,(j+1)*L_batch))\n",
    "        zi = paddle.to_tensor(feature_temp[i_ind])\n",
    "        fi = paddle.to_tensor(Probs_temp[i_ind])\n",
    "        yi = paddle.to_tensor(Y_onehot[i_ind])\n",
    "        zixzC = zi @ zC.transpose([1,0])\n",
    "        part_1_1 = (zixzC + 1) @ fCcyC\n",
    "        part1 = (part_1_1 * (yi-fi)).sum(axis=-1,keepdim=True)*4*lr/len(anchor_points)\n",
    "\n",
    "        learning_risk[i_ind] = part1.numpy().ravel()\n",
    "\n",
    "    select = np.zeros((samples,),dtype=np.bool8)\n",
    "    score = learning_risk\n",
    "    for j in range(n_class):\n",
    "        class_mask = Yt_list[-1] == j\n",
    "        c_n = class_mask.sum()\n",
    "        c_th = np.sort(score[class_mask])[-min(int(c_n * n_V),c_n)]\n",
    "        select[np.logical_and(score>=c_th, class_mask)] = True\n",
    "    if epoch_id < t_w:\n",
    "        temp_Yt_noised = Yt_list[-1]\n",
    "    else:\n",
    "        temp_Yt_noised=np.where(select.ravel(), Pred_temp.ravel(), Yt_list[-1].ravel()).astype(int)\n",
    "\n",
    "    score_list.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
