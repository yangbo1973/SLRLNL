{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f7299a-ed05-41d4-8524-0f84dc1c9afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:49.069121Z",
     "iopub.status.busy": "2023-04-25T09:12:49.065232Z",
     "iopub.status.idle": "2023-04-25T09:12:52.253199Z",
     "shell.execute_reply": "2023-04-25T09:12:52.252264Z",
     "shell.execute_reply.started": "2023-04-25T09:12:49.069036Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import MutableMapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Iterable, Mapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Sized\r\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.static import InputSpec\n",
    "from paddle.fluid.framework import core\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "import gc\n",
    "from itertools import repeat\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562de79e-d51c-4a81-a631-99225868dd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:52.261448Z",
     "iopub.status.busy": "2023-04-25T09:12:52.260088Z",
     "iopub.status.idle": "2023-04-25T09:12:52.273297Z",
     "shell.execute_reply": "2023-04-25T09:12:52.271081Z",
     "shell.execute_reply.started": "2023-04-25T09:12:52.261384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "samples = 50000\n",
    "test_samples = 10000\n",
    "examples = samples\n",
    "n_class = 100\n",
    "\n",
    "training_epochs = 100\n",
    "\n",
    "noise_type = 'uniform_flip' # can be set as 'pair_flip' or 'uniform_flip'\n",
    "noise_rate = 0.8 # the noise rate\n",
    "image_size = 112\n",
    "n_C = 0.1 # nearly clean samples selection parameter\n",
    "n_V = 0.2 # correction proportion\n",
    "n_R = 0.05 # relabeling proportion\n",
    "t_w = 3 # warm up epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a7f8fc-aa72-4aed-8c96-4d6b434de1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:52.277277Z",
     "iopub.status.busy": "2023-04-25T09:12:52.276736Z",
     "iopub.status.idle": "2023-04-25T09:12:59.765954Z",
     "shell.execute_reply": "2023-04-25T09:12:59.764058Z",
     "shell.execute_reply.started": "2023-04-25T09:12:52.277234Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download training data and load training data.\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import paddle.vision.transforms as T\n",
    "from paddle.vision.transforms import BaseTransform\n",
    "\n",
    "class Dataset(paddle.io.Dataset):\n",
    "    def __init__(self, num_samples, paddle_data, ):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.paddle_data = paddle_data\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.paddle_data[index][0]\n",
    "        data = np.array(image)\n",
    "\n",
    "        return index, data\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "mean,std = ([0.4914, 0.4822, 0.4465],[0.2471, 0.2435,0.2616])\n",
    "mean = np.array(mean).reshape(1,1,3)*255\n",
    "std = np.array(std).reshape(1,1,3)*255\n",
    "\n",
    "transform_robust = T.Compose([\n",
    "                    T.ColorJitter(),\n",
    "                    T.RandomHorizontalFlip(0.5),\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "transform_clean = T.Compose([\n",
    "                    # T.ColorJitter(),\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "\n",
    "print('Start download training data and load training data.')\n",
    "train_dataset_robust = paddle.vision.datasets.Cifar100(mode='train', transform=transform_robust, download = True)\n",
    "test_dataset = paddle.vision.datasets.Cifar100(mode='test', transform=transform_clean, download = True)\n",
    "\n",
    "train_data = Dataset(samples, train_dataset_robust)\n",
    "test_data = Dataset(test_samples, test_dataset)\n",
    "train_loaderrobust = paddle.io.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=6,)\n",
    "valid_loader = paddle.io.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6,)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d5fb76-860f-4a70-a62f-43fe7842e0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:59.768752Z",
     "iopub.status.busy": "2023-04-25T09:12:59.768085Z",
     "iopub.status.idle": "2023-04-25T09:12:59.785045Z",
     "shell.execute_reply": "2023-04-25T09:12:59.783128Z",
     "shell.execute_reply.started": "2023-04-25T09:12:59.768690Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train_Y = np.load(r'./work/cifar-100-train_Y.npy').reshape(-1, )\n",
    "    test_Y = np.load(r'./work/cifar-100-test_Y.npy').reshape(-1, )\n",
    "except :\n",
    "    train_Y = []\n",
    "    for d in tqdm.tqdm(train_dataset):\n",
    "        train_Y.append(d[1])\n",
    "    train_Y = np.array(train_Y).reshape(-1,)\n",
    "    np.save(r'./work/cifar-100-train_Y.npy', train_Y)\n",
    "    test_Y = []\n",
    "    for d in tqdm.tqdm(test_dataset):\n",
    "        test_Y.append(d[1])\n",
    "    test_Y = np.array(test_Y).reshape(-1,)\n",
    "    np.save(r'./work/cifar-100-test_Y.npy', test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e94bb9d-68bc-464e-8e8b-b1f7af4b49b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:59.787786Z",
     "iopub.status.busy": "2023-04-25T09:12:59.787003Z",
     "iopub.status.idle": "2023-04-25T09:12:59.798074Z",
     "shell.execute_reply": "2023-04-25T09:12:59.797164Z",
     "shell.execute_reply.started": "2023-04-25T09:12:59.787748Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "if noise_type == 'pair_flip':\n",
    "    noise_ = np.where(np.random.binomial(1, noise_rate, train_Y.shape), 1,0)\n",
    "    train_Y_noised = train_Y.copy()\n",
    "    for i in range(len(noise_)):\n",
    "        if noise_[i]>0:\n",
    "            if np.mod(train_Y_noised[i]+1,5) == 0:\n",
    "                train_Y_noised[i] -=4\n",
    "            else:\n",
    "                train_Y_noised[i] +=1\n",
    "elif noise_type == 'uniform_flip':\n",
    "    noise_=np.where(np.random.binomial(n=1,p=noise_rate,size=train_Y.shape),np.random.randint(low=1,high=n_class-1,size=train_Y.shape),0)\n",
    "    train_Y_noised = np.mod(noise_ + train_Y,n_class)\n",
    "    train_Y_noised = np.array(train_Y_noised)\n",
    "else:\n",
    "    raise ValueError(\"The noise_type must be 'uniform_flip' or 'pair_flip'.\")\n",
    "\n",
    "Yt_list = [train_Y_noised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec660f1-d872-4150-8a5c-f5c3eb5cbc62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:59.799455Z",
     "iopub.status.busy": "2023-04-25T09:12:59.799095Z",
     "iopub.status.idle": "2023-04-25T09:12:59.806370Z",
     "shell.execute_reply": "2023-04-25T09:12:59.805702Z",
     "shell.execute_reply.started": "2023-04-25T09:12:59.799431Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    loss_eval = 0\n",
    "    acc_eval = 0\n",
    "    network_3.eval()\n",
    "    Py_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    for batch_id, (ind,X_data) in enumerate(valid_loader()):\n",
    "        ind = np.array(ind)\n",
    "        with paddle.no_grad():\n",
    "            Y_data = np.array(test_Y[ind]).astype(np.int64)\n",
    "            X_GPU = paddle.to_tensor(X_data)\n",
    "            Y_GPU = paddle.to_tensor(Y_data)\n",
    "            y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "            logits = network_3(X_GPU)\n",
    "            probs = paddle.nn.functional.softmax(logits)\n",
    "            Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "            Py_temp[ind] = Py.numpy()\n",
    " \n",
    "            loss = loss_fn(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "            acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "        loss_eval += loss.numpy()\n",
    "        acc_eval += acc.numpy()\n",
    "\n",
    "    loss_eval/=(batch_id+1)\n",
    "    acc_eval/=(batch_id+1)\n",
    "    return loss_eval, acc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5bea8e-a032-4897-87ea-c0a5c7e15bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:59.807377Z",
     "iopub.status.busy": "2023-04-25T09:12:59.807159Z",
     "iopub.status.idle": "2023-04-25T09:12:59.811293Z",
     "shell.execute_reply": "2023-04-25T09:12:59.810645Z",
     "shell.execute_reply.started": "2023-04-25T09:12:59.807357Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Py_list = []\r\n",
    "acc_list = []\r\n",
    "loss_list = []\r\n",
    "warnings.filterwarnings(\"ignore\", category=Warning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8fd8b-bbde-436d-bb9a-e5acd6f176a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:12:59.812532Z",
     "iopub.status.busy": "2023-04-25T09:12:59.812185Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 17:12:59.835304 28524 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0425 17:12:59.839766 28524 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "782it [00:53, 14.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.7390, train acc:0.0215, train acc ori:0.0215,  eval loss:4.1781, eval acc:0.1605\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:49, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.5031, train acc:0.0608, train acc ori:0.0608,  eval loss:3.6519, eval acc:0.3195\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:49, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.3541, train acc:0.0949, train acc ori:0.0949,  eval loss:3.3522, eval acc:0.3766\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:51, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.0135, train acc:0.1731, train acc ori:0.1231,  eval loss:3.0239, eval acc:0.4001\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:51, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:3.3586, train acc:0.2746, train acc ori:0.1593,  eval loss:2.7054, eval acc:0.4082\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:52, 15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.5883, train acc:0.4099, train acc ori:0.1957,  eval loss:2.5404, eval acc:0.4138\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:51, 15.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.9600, train acc:0.5354, train acc ori:0.2285,  eval loss:2.4546, eval acc:0.4343\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:52, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train complete\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.5312, train acc:0.6196, train acc ori:0.2456,  eval loss:2.4407, eval acc:0.4441\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "457it [00:31, 11.72it/s]"
     ]
    }
   ],
   "source": [
    "network_3 = paddle.vision.models.resnet34(num_classes=n_class,pretrained=True)\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "opt = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=network_3.parameters(),)\n",
    "save_time = time.asctime()\n",
    "\n",
    "for epoch_id in range(training_epochs):\n",
    "    network_3.train()\n",
    "    loader = train_loaderrobust()\n",
    "\n",
    "    loss_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    feature_temp = np.zeros((samples, 512), dtype = np.float32)\n",
    "    \n",
    "    if len(Py_list) > 1:\n",
    "        score = Py_list[-1]\n",
    "    else:\n",
    "        score = np.random.rand(samples,)\n",
    "    OOD_mask = score < np.sort(score)[int(samples * n_R)]\n",
    "    \n",
    "    for batch_id, (ind,X_data) in tqdm.tqdm(enumerate(loader)):\n",
    "        ind = np.array(ind)\n",
    "        Y_data = np.array(Yt_list[-1][ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(Yt_list[0][ind]).astype(np.int64)\n",
    "        temp_X = paddle.to_tensor(X_data)\n",
    "        Y_GPU = paddle.to_tensor(Y_data)\n",
    "        Y_GPU_ori = paddle.to_tensor(Y_data_ori)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "        first = network_3.maxpool(network_3.relu(network_3.bn1(network_3.conv1(temp_X))))\n",
    "        l1 = network_3.layer1(first)\n",
    "        l2 = network_3.layer2(l1)\n",
    "        l3 = network_3.layer3(l2)\n",
    "        l4 = network_3.layer4(l3)\n",
    "        feature = paddle.reshape(network_3.avgpool(l4), (l4.shape[0],-1))\n",
    "        \n",
    "        logits = network_3.fc(feature)\n",
    "\n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pred = paddle.argmax(probs,axis=-1)\n",
    "        logits_other = logits - logits * y_onehot\n",
    "        Pred_other = paddle.argmax(logits_other,axis=-1)\n",
    "        if epoch_id < t_w:\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "        else:\n",
    "            Y_GPU = paddle.where(paddle.to_tensor(OOD_mask[ind]), Pred_other, Y_GPU)\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "            \n",
    "        Py_temp[ind] = Py.numpy()\n",
    "        Pred_temp[ind] = Pred.numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.numpy()\n",
    "        feature_temp[ind] = feature.numpy()\n",
    "        \n",
    "        acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "        acc_ori = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "        loss_ori = loss_fn(logits,paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "\n",
    "        loss_train += loss.numpy()\n",
    "        acc_train += acc.numpy()\n",
    "        acc_train_ori += acc_ori.numpy()\n",
    "        loss_train_ori += loss_ori.numpy()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        \n",
    "    loss_train/=(batch_id+1)\n",
    "    acc_train/=(batch_id+1)\n",
    "    acc_train_ori/=(batch_id+1)\n",
    "    loss_train_ori/=(batch_id+1)\n",
    "\n",
    "    print('epoch %d train complete'%epoch_id)\n",
    "    loss_eval, acc_eval = validation()\n",
    "\n",
    "\n",
    "    loss_list.append(loss_eval)\n",
    "    acc_list.append(acc_eval)\n",
    "    Py_list.append(Py_temp)\n",
    "    \n",
    "\n",
    "    print('train loss:%.4f, train acc:%.4f, train acc ori:%.4f,  eval loss:%.4f, eval acc:%.4f'\n",
    "    %(loss_train, acc_train, acc_train_ori, loss_eval, acc_eval))  \n",
    "\n",
    "\n",
    "\n",
    "    if epoch_id >= t_w:\n",
    "        nC_points = []\n",
    "        anchor_mask = np.zeros((samples,),dtype=bool)\n",
    "        Py_mean = np.zeros((samples,))\n",
    "        for j in range(len(Py_list)):\n",
    "            Py_mean+=Py_list[j]\n",
    "        Py_mean/=len(Py_list)   \n",
    "        for j in range(n_class):\n",
    "            class_mask = Yt_list[0] == j\n",
    "            c_n = class_mask.sum()\n",
    "            c_th = np.sort(Py_mean[class_mask])[-int(c_n * n_C)]\n",
    "            nC_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "        nC_points = np.concatenate(nC_points)\n",
    "        L_batch = 1000\n",
    "        Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "        zC = paddle.to_tensor(feature_temp[nC_points])\n",
    "        fC = paddle.to_tensor(Probs_temp[nC_points])\n",
    "        yC = paddle.to_tensor(Y_onehot[nC_points])\n",
    "        fCcyC = fC - yC\n",
    "        lr = 1e-6\n",
    "        learning_risk = np.zeros((samples,)) \n",
    "        for j in range(int(np.ceil(samples/L_batch))):\n",
    "            i_ind = np.arange(j*L_batch, min(samples,(j+1)*L_batch))\n",
    "            zi = paddle.to_tensor(feature_temp[i_ind])\n",
    "            fi = paddle.to_tensor(Probs_temp[i_ind])\n",
    "            yi = paddle.to_tensor(Y_onehot[i_ind])\n",
    "            zixzC = zi @ zC.transpose([1,0])\n",
    "            part_1_1 = (zixzC + 1) @ fCcyC\n",
    "            part1 = (part_1_1 * (yi-fi)).sum(axis=-1,keepdim=True)*4*lr/len(nC_points)\n",
    "            part1_all = part_1_1 * (fi-paddle.ones_like(yi))*4*lr/len(nC_points)\n",
    "\n",
    "            learning_risk[i_ind] = part1.numpy().ravel()\n",
    "\n",
    "        select = np.zeros((samples,),dtype=np.bool8)\n",
    "        for j in range(n_class):\n",
    "            class_mask = Yt_list[-1] == j\n",
    "            c_n = class_mask.sum()\n",
    "            c_th = np.sort(learning_risk[class_mask])[-min(int(c_n * n_V),c_n)]\n",
    "            select[np.logical_and(learning_risk>=c_th, class_mask)] = True\n",
    "        \n",
    "        temp_Yt_noised=np.where(select.ravel(), Pred_temp.ravel(), Yt_list[-1].ravel()).astype(int)\n",
    "        Yt_list.append(temp_Yt_noised)\n",
    "\n",
    "\n",
    "    with open(r'./result/cifar100_acc_eval%s.txt'%save_time,'a') as f:\n",
    "        f.write('%.8f\\n'%acc_eval)\n",
    "    with open(r'./result/cifar100_loss_eval%s.txt'%save_time,'a') as f:\n",
    "        f.write('%.8f\\n'%loss_eval)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe6044-bac8-40e5-a9b4-98b79fcca011",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c5049-3601-478c-824b-79e57a478487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3901670-97e4-437b-aa7a-ac47dbe1b7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e6e65-5daa-470f-91a1-20c40a85b3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02306a66-6cb2-4d3e-96da-99d97ca73365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685dfe5-d899-416f-8848-9d35ed5997ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45454f21-2e4d-4549-8efb-87f00323bd34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e97304-ebaf-4caf-9d45-2c663089de68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462ba7b-0511-4541-8a76-1d0436f95342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdadd3-714f-4e34-a14e-1a19aed799d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc2dfc-b618-44e5-b8ef-a6febf18a369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db14a56-ebc3-46e0-a8a2-6352ad9859d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
