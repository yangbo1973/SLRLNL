{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:33.027992Z",
     "iopub.status.busy": "2023-04-25T14:11:33.027630Z",
     "iopub.status.idle": "2023-04-25T14:11:35.235163Z",
     "shell.execute_reply": "2023-04-25T14:11:35.233843Z",
     "shell.execute_reply.started": "2023-04-25T14:11:33.027965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import shelve\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import paddle\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:35.237893Z",
     "iopub.status.busy": "2023-04-25T14:11:35.237199Z",
     "iopub.status.idle": "2023-04-25T14:11:38.290341Z",
     "shell.execute_reply": "2023-04-25T14:11:38.289214Z",
     "shell.execute_reply.started": "2023-04-25T14:11:35.237858Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./work/word_to_ind.json') as f:\n",
    "    word_to_ind = json.load(f)\n",
    "ind_to_word = {i: w for w, i in word_to_ind.items()}\n",
    "with open('./work/rel_to_ind.json') as f:\n",
    "    rel_to_ind = json.load(f)\n",
    "ind_to_rel = {i: w for w, i in rel_to_ind.items()}\n",
    "with open('./work/type_to_ind.json') as f:\n",
    "    type_to_ind = json.load(f)\n",
    "with open('./work/distant_dict.json') as f:\n",
    "    distant_dict = json.load(f)\n",
    "    distant_dict = set((l[0], l[1]) for l in distant_dict)\n",
    "ind_to_type = {i: w for w, i in type_to_ind.items()}\n",
    "\n",
    "glove_file = r'data/data92342/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.291966Z",
     "iopub.status.busy": "2023-04-25T14:11:38.291677Z",
     "iopub.status.idle": "2023-04-25T14:11:38.297712Z",
     "shell.execute_reply": "2023-04-25T14:11:38.296892Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.291940Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "\n",
    "n_class = len(rel_to_ind)\n",
    "\n",
    "no_components = 300\n",
    "\n",
    "ner_emb = 30\n",
    "coref_emb = 30\n",
    "dep_emb = 30\n",
    "sent_dist_emb = 30\n",
    "dim_sent = 30\n",
    "\n",
    "max_sents = 21\n",
    "\n",
    "coref_maxlen = 60\n",
    "\n",
    "sent_rel_max = 1800\n",
    "token_max_len = 511\n",
    "max_seq_len = 511\n",
    "\n",
    "dim = [500, 250, 128, 105]\n",
    "dim_2 = 60\n",
    "drop_out = 0.5\n",
    "\n",
    "learning_rate = 2e-4\n",
    "epoch_n = 100\n",
    "\n",
    "n_C = 0.1 # nearly clean samples selection parameter\n",
    "n_V = 0.01 # correction proportion\n",
    "n_R = 0.01 # relabeling proportion\n",
    "t_w = 30 # warm up epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.300134Z",
     "iopub.status.busy": "2023-04-25T14:11:38.299778Z",
     "iopub.status.idle": "2023-04-25T14:11:38.654613Z",
     "shell.execute_reply": "2023-04-25T14:11:38.653521Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.300109Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: DeprecationWarning: invalid escape sequence \\.\r\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\.\r\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\.\r\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\.\r\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\.\r\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\.\r\n",
      "/tmp/ipykernel_2114/3500270483.py:16: DeprecationWarning: invalid escape sequence \\.\r\n",
      "  elif re.split('\\.', glove_file)[-1] == 'txt':\r\n",
      "/tmp/ipykernel_2114/3500270483.py:4: DeprecationWarning: invalid escape sequence \\.\r\n",
      "  if re.split('\\.', glove_file)[-1] == 'bin':\r\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "if not os.path.exists('./work/glove_emb_mat.npy'):\n",
    "    glove_emb_mat = (np.random.rand(len(word_to_ind) + 2, no_components) - 0.5)\n",
    "    if re.split('\\.', glove_file)[-1] == 'bin':\n",
    "        from gensim.models import KeyedVectors\n",
    "        from gensim.test.utils import datapath\n",
    "        import gc\n",
    "\n",
    "        wv_from_bin = KeyedVectors.load_word2vec_format(datapath(glove_file), binary=True)\n",
    "        for w, i in tqdm.tqdm(word_to_ind.items()):\n",
    "            if w in wv_from_bin:\n",
    "                glove_emb_mat[i, :] = wv_from_bin.get_vector(w)\n",
    "        del wv_from_bin\n",
    "        gc.collect()\n",
    "        np.save('./work/glove_emb_mat.npy', glove_emb_mat)\n",
    "    elif re.split('\\.', glove_file)[-1] == 'txt':\n",
    "        with open(glove_file, 'r', encoding='utf8') as f:\n",
    "            for line in tqdm.tqdm(f):\n",
    "                w, *vec = re.split(' ', line)\n",
    "                if w in word_to_ind.keys():\n",
    "                    glove_emb_mat[word_to_ind[w], :] = [float(v) for v in vec[:no_components]]\n",
    "        np.save('./work/glove_emb_mat.npy', glove_emb_mat)\n",
    "else:\n",
    "    glove_emb_mat = np.load('./work/glove_emb_mat.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.656587Z",
     "iopub.status.busy": "2023-04-25T14:11:38.655892Z",
     "iopub.status.idle": "2023-04-25T14:11:38.661236Z",
     "shell.execute_reply": "2023-04-25T14:11:38.660293Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.656556Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dis2idx = np.zeros((max_seq_len,), dtype=np.int64)\n",
    "dis2idx[1] = 1\n",
    "for i in range(2, 10):\n",
    "    dis2idx[(2 ** (i - 1)):] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.663084Z",
     "iopub.status.busy": "2023-04-25T14:11:38.662749Z",
     "iopub.status.idle": "2023-04-25T14:11:38.690194Z",
     "shell.execute_reply": "2023-04-25T14:11:38.689200Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.663056Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def dump_data(raw_datas,save_path,y_path):\n",
    "    data = []\n",
    "    for temp in raw_datas:\n",
    "        if sum(len(st) for st in temp['sents']) < token_max_len:\n",
    "            data.append(temp)\n",
    "    # example_data = []\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    if os.path.exists(y_path):\n",
    "        os.remove(y_path)\n",
    "    \n",
    "    with open(y_path,'ab') as f_y:\n",
    "        with open(save_path, 'ab') as f:\n",
    "            save_index = 0\n",
    "#         with open(save_path,'ab') as f:\n",
    "\n",
    "            for index, temp in tqdm.tqdm(enumerate(data)):\n",
    "                title = temp['title']\n",
    "                sent_start_pos = np.cumsum([0] + [len(st) for st in temp['sents']])\n",
    "                sent_org_token = [t for st in temp['sents'] for t in st]\n",
    "                sent_coref_id = np.zeros((token_max_len,))\n",
    "                sent_ner_id = np.zeros((token_max_len,))\n",
    "                for idx, en in enumerate(temp['vertexSet']):\n",
    "                    for ment in en:\n",
    "                        em_pos = np.array(ment['pos']) + sent_start_pos[ment['sent_id']]\n",
    "                        sent_coref_id[em_pos[0]:em_pos[1]] = idx + 1\n",
    "                        sent_ner_id[em_pos[0]:em_pos[1]] = type_to_ind[ment['type']] + 1\n",
    "                sent_coref_id = sent_coref_id.astype(int)\n",
    "                sent_ner_id = sent_ner_id.astype(int)\n",
    "\n",
    "                sent_glove_id = []\n",
    "                for token in sent_org_token:\n",
    "                    sent_glove_id.append(word_to_ind[token] if token in word_to_ind.keys() else 1)\n",
    "                while len(sent_glove_id) < token_max_len:\n",
    "                    sent_glove_id.append(0)\n",
    "                sent_glove_id = np.array(sent_glove_id)\n",
    "                rels = []\n",
    "                entity_n = len(temp['vertexSet'])\n",
    "                \n",
    "                h_t_dict = {}\n",
    "                for rel in temp['labels']:\n",
    "                    # label_names.append(rel['r'])\n",
    "                    if (rel['h'], rel['t']) not in h_t_dict.keys():\n",
    "                        h_t_dict[(rel['h'], rel['t'])] = [rel['r']]\n",
    "                    else:\n",
    "                        h_t_dict[(rel['h'], rel['t'])].append(rel['r'])\n",
    "\n",
    "                neg_set = []\n",
    "                for i in range(entity_n):\n",
    "                    for j in range(entity_n):\n",
    "                        if i != j:\n",
    "                            neg_set.append((i, j))\n",
    "                if len(neg_set) > sent_rel_max:\n",
    "                    np.random.shuffle(neg_set)\n",
    "                neg_set = set(neg_set[:sent_rel_max])\n",
    "                \n",
    "                for h_i in range(entity_n):\n",
    "                    for t_i in range(entity_n):\n",
    "                        if h_i != t_i:\n",
    "                            if (is_train and ((h_i, t_i) in h_t_dict.keys() or (h_i, t_i) in neg_set)) or is_validation:\n",
    "                                h_pos = []\n",
    "                                h_map_v = []\n",
    "                                h_count = Counter([m['type'] for m in temp['vertexSet'][h_i]])\n",
    "                                h_count_max = max(h_count.values())\n",
    "                                h_type = [w for w in h_count if h_count[w] == h_count_max][0]\n",
    "                                h_type_index = type_to_ind[h_type]\n",
    "                                # type_names.append(h_type)\n",
    "\n",
    "                                t_pos = []\n",
    "                                t_map_v = []\n",
    "                                t_count = Counter([m['type'] for m in temp['vertexSet'][t_i]])\n",
    "                                t_count_max = max(t_count.values())\n",
    "                                t_type = [w for w in t_count if t_count[w] == t_count_max][0]\n",
    "                                t_type_index = type_to_ind[t_type]\n",
    "\n",
    "\n",
    "                                # type_names.append(t_type)\n",
    "\n",
    "                                label_indexs = []\n",
    "\n",
    "                                delta_dis = temp['vertexSet'][h_i][0]['pos'][0] + \\\n",
    "                                            sent_start_pos[temp['vertexSet'][h_i][0]['sent_id']] - \\\n",
    "                                            temp['vertexSet'][t_i][0]['pos'][0] - \\\n",
    "                                            sent_start_pos[temp['vertexSet'][t_i][0]['sent_id']]\n",
    "                                sent_dist = dis2idx[delta_dis] if delta_dis > 0 else -dis2idx[-delta_dis]\n",
    "\n",
    "                                for h in temp['vertexSet'][h_i]:\n",
    "                                    for t in temp['vertexSet'][t_i]:\n",
    "                                        em1_pos = np.array(h['pos']) + sent_start_pos[h['sent_id']]\n",
    "                                        em1_pos_num = list(range(em1_pos[0], em1_pos[1]))\n",
    "                                        h_pos.extend(em1_pos_num)\n",
    "                                        em1_pos_num = list(range(em1_pos[0], em1_pos[1]))\n",
    "                                        em1_map_value = [1 / len(em1_pos_num) / len(temp['vertexSet'][h_i]) for i in\n",
    "                                                        range(em1_pos[1] - em1_pos[0])]\n",
    "                                        h_map_v.extend(em1_map_value)\n",
    "\n",
    "                                        em2_pos = np.array(t['pos']) + sent_start_pos[t['sent_id']]\n",
    "                                        em2_pos_num = list(range(em2_pos[0], em2_pos[1]))\n",
    "                                        t_pos.extend(em2_pos_num)\n",
    "                                        em2_pos_num = list(range(em2_pos[0], em2_pos[1]))\n",
    "                                        em2_map_value = [1 / len(em2_pos_num) / len(temp['vertexSet'][t_i]) for i in\n",
    "                                                        range(em2_pos[1] - em2_pos[0])]\n",
    "                                        t_map_v.extend(em2_map_value)                                        \n",
    "                                labels = h_t_dict[(h_i, t_i)] if (h_i, t_i) in h_t_dict.keys() else ['None']\n",
    "                                for label in labels:\n",
    "                                    label_indexs.append(rel_to_ind[label])\n",
    "\n",
    "                                temp_rel = {'h_pos': h_pos,\n",
    "                                            'h_map_v': h_map_v,\n",
    "                                            'h_type_index':h_type_index,\n",
    "                                            't_pos': t_pos,\n",
    "                                            't_map_v': t_map_v,\n",
    "                                            't_type_index':t_type_index,\n",
    "                                            'sent_dist': sent_dist,\n",
    "                                            'label_indexs':label_indexs[0]}\n",
    "                                rels.append(temp_rel)\n",
    "                # if len(rels) > 0 and any(r_>0 for r in rels for r_ in r['label_indexs']):\n",
    "                if len(rels) > 0 and any(r['label_indexs']>0 for r in rels ):\n",
    "                    example = {'title':title,\n",
    "                    'index':index,\n",
    "                    'sent_glove_id':sent_glove_id,\n",
    "                    'sent_coref_id':sent_coref_id,\n",
    "                    'sent_ner_id':sent_ner_id,\n",
    "                    'rels':rels}\n",
    "                    \n",
    "                    pickle.dump(example,f)\n",
    "                    pickle.dump([r['label_indexs'] for r in rels],f_y)\n",
    "                    \n",
    "                    \n",
    "                    # f.write(pickle.dumps(example)+'\\n')\n",
    "                    # f_y.write(pickle.dumps([r['label_indexs'] for r in rels])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.692125Z",
     "iopub.status.busy": "2023-04-25T14:11:38.691476Z",
     "iopub.status.idle": "2023-04-25T14:11:38.696585Z",
     "shell.execute_reply": "2023-04-25T14:11:38.695822Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.692094Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(r'data/data92314/train_annotated.json') as f:\n",
    "#     raw = json.load(f)\n",
    "# dump_data(raw,'./work/train_annotated_data.pkl','./work/train_annotated_y.pkl')\n",
    "\n",
    "# with open('./train_distant.json') as f:\n",
    "#     raw = json.load(f)\n",
    "# dump_data(raw,'./train_distant_data','./train_distant_y.pkl')\n",
    "# # # dump_data(raw[:int(len(raw)/2)],'/home/aistudio/work/train_distant_data1.pkl','/home/aistudio/work/train_distant_y1.pkl')\n",
    "# # # dump_data(raw[int(len(raw)/2):],'/home/aistudio/work/train_distant_data2.pkl','/home/aistudio/work/train_distant_y2.pkl')\n",
    "\n",
    "# with open('data/data92314/dev.json') as f:\n",
    "#     raw = json.load(f)\n",
    "# dump_data(raw,'./work/dev_data.pkl','./work/dev_y.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.698645Z",
     "iopub.status.busy": "2023-04-25T14:11:38.697753Z",
     "iopub.status.idle": "2023-04-25T14:11:38.895344Z",
     "shell.execute_reply": "2023-04-25T14:11:38.894120Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.698614Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_annotated_y = []\n",
    "with open('./work/train_annotated_y.pkl','rb') as f:\n",
    "    while 1:\n",
    "        try:\n",
    "            temp = pickle.load(f)\n",
    "            train_annotated_y.append(temp)\n",
    "        except EOFError:\n",
    "            break\n",
    "# train_distant_y = []\n",
    "# with open('./train_distant_y.pkl','rb') as f:\n",
    "#     while 1:\n",
    "#         try:\n",
    "#             temp = pickle.load(f)\n",
    "#             train_distant_y.append(temp)\n",
    "#         except EOFError:\n",
    "#             break\n",
    "dev_y = []\n",
    "with open('./work/dev_y.pkl','rb') as f:\n",
    "    while 1:\n",
    "        try:\n",
    "            temp = pickle.load(f)\n",
    "            dev_y.append(temp)\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "train_annotated_y_start = np.cumsum([0] + [len(d) for d in train_annotated_y])\n",
    "train_annotated_y = np.array([y for d in train_annotated_y for y in d])\n",
    "\n",
    "dev_y_start = np.cumsum([0] + [len(d) for d in dev_y])\n",
    "dev_y = np.array([y for d in dev_y for y in d])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.897479Z",
     "iopub.status.busy": "2023-04-25T14:11:38.896761Z",
     "iopub.status.idle": "2023-04-25T14:11:38.903688Z",
     "shell.execute_reply": "2023-04-25T14:11:38.902745Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.897444Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_random_label(train_Y, label_account = None):\n",
    "    if label_account == None:\n",
    "        noise_int = np.random.randint(low=0,high=n_class-1,size=train_Y.shape)\n",
    "        train_Y_rand =  np.mod(noise_int + train_Y,n_class)\n",
    "        train_Y_rand = np.array(train_Y_rand)\n",
    "    else:\n",
    "        dis_prob = [label_account[i]/sum(j for j in label_account.values()) for i in label_account]\n",
    "        train_Y_rand = np.random.choice([i for i in label_account.keys()],train_Y.shape[0],p=dis_prob)\n",
    "    return train_Y_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.907289Z",
     "iopub.status.busy": "2023-04-25T14:11:38.906784Z",
     "iopub.status.idle": "2023-04-25T14:11:38.917446Z",
     "shell.execute_reply": "2023-04-25T14:11:38.916430Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.907259Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data_new(f, ):    \n",
    "    def data_generator():\n",
    "        start = 0\n",
    "        i=0\n",
    "        temp_glove_id = []\n",
    "        temp_coref_id = []\n",
    "        temp_ner_id = []\n",
    "        rels_list = []\n",
    "        while 1:\n",
    "            try:\n",
    "                d = pickle.load(f)\n",
    "                if (i==start or (i-start)%batch_size!=0):\n",
    "                    temp_glove_id.append(np.array(d['sent_glove_id']).reshape(1,-1))\n",
    "                    temp_coref_id.append(np.array(d['sent_coref_id']).reshape(1,-1))\n",
    "                    temp_ner_id.append(np.array(d['sent_ner_id']).reshape(1,-1))\n",
    "                    rels_list.append(d['rels'])\n",
    "                else:\n",
    "                    yield np.concatenate(temp_glove_id,axis=0).astype(np.int32),np.concatenate(temp_coref_id,axis=0).astype(np.int32),np.concatenate(temp_ner_id,axis=0).astype(np.int32),rels_list\n",
    "                    temp_glove_id = [np.array(d['sent_glove_id']).reshape(1,-1)]\n",
    "                    temp_coref_id = [np.array(d['sent_coref_id']).reshape(1,-1)]\n",
    "                    temp_ner_id = [np.array(d['sent_ner_id']).reshape(1,-1)]\n",
    "                    rels_list = [d['rels']]\n",
    "                i+=1\n",
    "            except EOFError:\n",
    "                if len(temp_glove_id)>0:\n",
    "                    yield np.concatenate(temp_glove_id,axis=0).astype(np.int32),np.concatenate(temp_coref_id,axis=0).astype(np.int32),np.concatenate(temp_ner_id,axis=0).astype(np.int32),rels_list\n",
    "                break\n",
    "\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:38.919574Z",
     "iopub.status.busy": "2023-04-25T14:11:38.919243Z",
     "iopub.status.idle": "2023-04-25T14:11:40.688908Z",
     "shell.execute_reply": "2023-04-25T14:11:40.687837Z",
     "shell.execute_reply.started": "2023-04-25T14:11:38.919546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 22:11:38.942303  2114 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0425 22:11:38.946472  2114 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n"
     ]
    }
   ],
   "source": [
    "class RE_model(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(RE_model, self).__init__()\n",
    "        self.word_embedding = paddle.nn.Embedding(glove_emb_mat.shape[0],glove_emb_mat.shape[1],padding_idx=0)\n",
    "        self.word_embedding.weight.set_value(glove_emb_mat.astype(np.float32))\n",
    "        self.coref_emb_mat = paddle.nn.Embedding(coref_maxlen, coref_emb,padding_idx=0)\n",
    "        self.ner_emb_mat = paddle.nn.Embedding(len(type_to_ind) + 1,ner_emb,padding_idx=0)\n",
    "        self.seq_lstm = paddle.nn.LSTM(glove_emb_mat.shape[1]+coref_emb+ner_emb,dim[2],direction='bidirectional')\n",
    "        self.dropout = paddle.nn.Dropout(p=drop_out)\n",
    "        self.head_linear = paddle.nn.Linear(2 * dim[2], dim[2])\n",
    "        self.tail_linear = paddle.nn.Linear(2 * dim[2], dim[2])\n",
    "        self.head_type_emb = paddle.nn.Embedding(len(type_to_ind), dim[2])\n",
    "        self.head_type_emb.weight.set_value(np.zeros((len(type_to_ind),dim[2])).astype(np.float32))\n",
    "        self.tail_type_emb = paddle.nn.Embedding(len(type_to_ind), dim[2])\n",
    "        self.tail_type_emb.weight.set_value(np.zeros((len(type_to_ind),dim[2])).astype(np.float32))\n",
    "\n",
    "        self.sent_dist_emb = paddle.nn.Embedding(max_sents,sent_dist_emb)\n",
    "\n",
    "        self.bilinear = paddle.nn.Bilinear(dim[2]+sent_dist_emb, dim[2]+sent_dist_emb, dim[3])\n",
    "\n",
    "        self.final_linear = paddle.nn.Linear(dim[3],len(rel_to_ind))\n",
    "        self.selu = paddle.nn.SELU()\n",
    "        self.relu = paddle.nn.ReLU()\n",
    "\n",
    "    def forward(self,\n",
    "                word_id,\n",
    "                coref_id,\n",
    "                ner_id,\n",
    "                h_pos,h_v,\n",
    "                t_pos,t_v,\n",
    "                rel_n,\n",
    "                h_type_index,\n",
    "                t_type_index,\n",
    "                ht_dist_index,\n",
    "                th_dist_index,):\n",
    "        temp_batch_size = word_id.shape[0]\n",
    "        \n",
    "        seq_emb = self.seq_lstm(paddle.concat([self.word_embedding(word_id),\n",
    "                                               self.coref_emb_mat(coref_id), \n",
    "                                               self.ner_emb_mat(ner_id)],axis=-1))\n",
    "        seq_emb = paddle.reshape(self.dropout(seq_emb[0]), (-1, 2 * dim[2]))\n",
    "        \n",
    "        head_seq_emb = self.relu(self.head_linear(seq_emb))\n",
    "        tail_seq_emb = self.relu(self.tail_linear(seq_emb))\n",
    "        \n",
    "        h_mask = paddle.scatter_nd(h_pos,paddle.to_tensor(h_v,dtype='float32'),shape=(rel_n,temp_batch_size * max_seq_len))\n",
    "        t_mask = paddle.scatter_nd(t_pos,paddle.to_tensor(t_v,dtype='float32'),shape=(rel_n,temp_batch_size * max_seq_len))\n",
    "        \n",
    "        h_temp = paddle.matmul(h_mask, head_seq_emb)\n",
    "        t_temp = paddle.matmul(t_mask, tail_seq_emb)\n",
    "\n",
    "        temp_head_bias = self.head_type_emb(h_type_index)\n",
    "        temp_tail_bias = self.tail_type_emb(t_type_index)\n",
    "\n",
    "        ht_dist_temp = self.sent_dist_emb(ht_dist_index)\n",
    "        th_dist_temp = self.sent_dist_emb(th_dist_index)\n",
    "\n",
    "        h_temp = paddle.concat([h_temp + temp_head_bias, ht_dist_temp],axis=1)\n",
    "        t_temp = paddle.concat([t_temp + temp_tail_bias, th_dist_temp],axis=1)\n",
    "        \n",
    "        sub_temp = self.selu(self.bilinear(h_temp,t_temp))\n",
    "        m_temp = self.final_linear(sub_temp)\n",
    "        \n",
    "        return m_temp, sub_temp\n",
    "\n",
    "class Orthogonal_loss17(paddle.nn.Layer):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss17, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.shape[0]\n",
    "        m = x.shape[1]\n",
    "\n",
    "        I = paddle.eye(m)\n",
    "        e = x - x.mean(axis=0, keepdim = True)\n",
    "        m_nonz = (e.sum(axis = 0) != 0).sum()\n",
    "        \n",
    "        cov = e.t() @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        # cov2 = paddle.abs(cov)\n",
    "        \n",
    "        select_i = paddle.argmax(cov2 - cov2 * I, axis = 1)\n",
    "        cov_m = (paddle.nn.functional.one_hot(select_i, m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / m_nonz / n\n",
    "        return result\n",
    "input_word = InputSpec((-1,max_seq_len), np.int32, 'word')\n",
    "input_coref = InputSpec((-1,max_seq_len), np.int32, 'coref')\n",
    "input_ner = InputSpec((-1,max_seq_len), np.int32, 'ner')\n",
    "input_h_pos = InputSpec((-1,2), np.int32, 'h_pos')\n",
    "input_h_v = InputSpec((-1,), np.float, 'h_v')\n",
    "input_t_pos = InputSpec((-1,2), np.int32, 't_pos')\n",
    "input_t_v = InputSpec((-1,), np.float32, 't_v')\n",
    "input_rel_n = InputSpec((1,), np.int32, 'rel_n')\n",
    "input_h_type = InputSpec((-1,), np.int32, 'h_type')\n",
    "input_t_type = InputSpec((-1,), np.int32, 't_type')\n",
    "input_ht = InputSpec((-1,), np.int32, 'ht')\n",
    "input_th = InputSpec((-1,), np.int32, 'th')\n",
    "input_label = InputSpec((-1,), np.int32, 'label')\n",
    "re_network1 = RE_model()\n",
    "cold_loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "sch = paddle.optimizer.lr.MultiStepDecay(learning_rate=learning_rate, milestones=[30, 50], gamma=0.1)\n",
    "opt = paddle.optimizer.Adam(learning_rate=sch,\n",
    "                            parameters=re_network1.parameters(),\n",
    "                            epsilon=1e-06, \n",
    "                            grad_clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:40.691117Z",
     "iopub.status.busy": "2023-04-25T14:11:40.690441Z",
     "iopub.status.idle": "2023-04-25T14:11:40.711594Z",
     "shell.execute_reply": "2023-04-25T14:11:40.710587Z",
     "shell.execute_reply.started": "2023-04-25T14:11:40.691087Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    r = np.corrcoef(x.T)\n",
    "\n",
    "    r = r ** 2\n",
    "    r[np.isnan(r)] = 0.0\n",
    "\n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))\n",
    "def transform_mydata(glove_id,coref_id,ner_id,rels_list):\n",
    "    start_i_list = [0] + np.cumsum([len(rels) for rels in rels_list])[:-1].tolist()\n",
    "    rel_n = np.reshape(np.sum([len(rels) for rels in rels_list]).astype(np.int32),(1,))\n",
    "    h_i = [start_i_list[batch_id] + i for batch_id, rels in enumerate(rels_list) for i,rel in enumerate(rels) for _ in rel['h_pos']]\n",
    "    h_j = [batch_id * max_seq_len + p for batch_id, rels in enumerate(rels_list) for rel in rels for p in rel['h_pos']]\n",
    "    # h_v = [1./len(rel['h_pos']) for batch_id, rels in enumerate(rels_batch) for rel in rels for p in rel['h_pos']]\n",
    "    h_v = np.array([v for batch_id, rels in enumerate(rels_list) for rel in rels for v in rel['h_map_v']])\n",
    "    t_i = [start_i_list[batch_id] + i for batch_id, rels in enumerate(rels_list) for i,rel in enumerate(rels) for _ in rel['t_pos']]\n",
    "    t_j = [batch_id * max_seq_len + p for batch_id, rels in enumerate(rels_list) for rel in rels for p in rel['t_pos']]\n",
    "    t_v = np.array([v for batch_id, rels in enumerate(rels_list) for rel in rels for v in rel['t_map_v']])\n",
    "    h_pos = np.array([h_i,h_j],dtype=np.int32).T\n",
    "    t_pos = np.array([t_i,t_j],dtype=np.int32).T\n",
    "    h_type = np.array([rel['h_type_index'] for batch_id, rels in enumerate(rels_list) for rel in rels],dtype=np.int32)\n",
    "    t_type = np.array([rel['t_type_index'] for batch_id, rels in enumerate(rels_list) for rel in rels],dtype=np.int32)\n",
    "    ht = np.array([rel['sent_dist'] + (max_sents-1)/2 for batch_id, rels in enumerate(rels_list) for rel in rels],dtype=np.int32)\n",
    "    th = np.array([-rel['sent_dist'] + (max_sents-1)/2 for batch_id, rels in enumerate(rels_list) for rel in rels],dtype=np.int32)\n",
    "    return [paddle.to_tensor(glove_id.astype(np.int32)),\n",
    "            paddle.to_tensor(coref_id.astype(np.int32)),\n",
    "            paddle.to_tensor(ner_id.astype(np.int32)),\n",
    "            paddle.to_tensor(h_pos),\n",
    "            paddle.to_tensor(h_v),\n",
    "            paddle.to_tensor(t_pos),\n",
    "            paddle.to_tensor(t_v),\n",
    "            paddle.to_tensor(rel_n),\n",
    "            paddle.to_tensor(h_type),\n",
    "            paddle.to_tensor(t_type),\n",
    "            paddle.to_tensor(ht),\n",
    "            paddle.to_tensor(th)]    \n",
    "def validation_new(model):\n",
    "    dev_f = open('./work/dev_data.pkl','rb')\n",
    "    dev_loader = load_data_new(dev_f,)\n",
    "    model.eval()\n",
    "    loss_eval = 0\n",
    "    f1_eval = 0\n",
    "    total_y_list = []\n",
    "    pred_y_list = []\n",
    "    loss_b_eval = 0\n",
    "    for batch_id, data in enumerate(dev_loader()):\n",
    "        with paddle.no_grad():\n",
    "            glove_id,coref_id,ner_id,rels_list = data\n",
    "            mydata = transform_mydata(glove_id,coref_id,ner_id,rels_list)\n",
    "            y_cpu = dev_y[dev_y_start[batch_id * batch_size] : dev_y_start[min((batch_id+1)*batch_size, len(dev_y_start)-1)]].astype(np.int64)\n",
    "            y_data = paddle.to_tensor(y_cpu)\n",
    "            y_onehot = paddle.nn.functional.one_hot(paddle.reshape(y_data,(-1,)),num_classes=n_class)\n",
    "            logits, feature = model(*mydata)\n",
    "            loss_b = relevant_hard_np(feature.numpy())\n",
    "            try:\n",
    "                _loss = cold_loss_fn(logits,y_data)\n",
    "            except:\n",
    "                _loss = cold_loss_fn(logits,y_onehot)\n",
    "            pred_y = np.argmax(logits.numpy(),axis=-1)\n",
    "\n",
    "        total_y_list.append(y_cpu)\n",
    "        pred_y_list.append(pred_y)\n",
    "\n",
    "        loss_eval += _loss.numpy()\n",
    "        loss_b_eval += loss_b\n",
    "    \n",
    "    total_y = np.concatenate(total_y_list,axis=0)\n",
    "    pred_y_total = np.concatenate(pred_y_list,axis=0)\n",
    "\n",
    "    correct_n = sum(np.logical_and(total_y == pred_y_total, total_y!=0))\n",
    "    pred_n = sum(pred_y_total!=0)\n",
    "    total_n = sum(total_y!=0)\n",
    "    \n",
    "    precision = correct_n / total_n\n",
    "    recall = correct_n / pred_n\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    loss_eval/=(batch_id+1)\n",
    "    loss_b_eval/=(batch_id+1)\n",
    "    \n",
    "    print('loss:%.4f, f1:%.4f, recall:%.4f, precision:%.4f, loss_b:%.4f'%(loss_eval,f1,recall,precision,loss_b_eval))\n",
    "\n",
    "    return loss_eval,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:40.713933Z",
     "iopub.status.busy": "2023-04-25T14:11:40.713144Z",
     "iopub.status.idle": "2023-04-25T14:11:40.740916Z",
     "shell.execute_reply": "2023-04-25T14:11:40.740072Z",
     "shell.execute_reply.started": "2023-04-25T14:11:40.713901Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_predict_model(loader,Yt_list,y_start,model,acc_list,):\n",
    "    model.train()\n",
    "    Py_temp = np.zeros((y_start[-1],),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((y_start[-1],),dtype=np.float32) \n",
    "    Probs_temp = np.zeros((y_start[-1],n_class),dtype=np.float32)\n",
    "    feature_temp = np.zeros((y_start[-1],dim[3]),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((y_start[-1],n_class),dtype=np.float32)\n",
    "\n",
    "    if len(Py_list)>0:\n",
    "        Py_mean = Py_list[-1]\n",
    "    else:\n",
    "        Py_mean = np.zeros((y_start[-1],))\n",
    "        \n",
    "    OOD_mask = np.zeros((y_start[-1],),dtype=np.bool)\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = Yt_list[0]==j_\n",
    "        class_n = class_mask.sum()\n",
    "        class_thres = np.sort(Py_mean[class_mask])[int((class_n-1)*0.01)]\n",
    "        OOD_mask[np.logical_and(class_mask, Py_mean<= class_thres)]=True\n",
    "\n",
    "    is_neg = (Yt_list[-1] == 0).ravel()   \n",
    "    for batch_id, data in enumerate(loader()):\n",
    "        mydata = transform_mydata(*data)\n",
    "        y_cpu = Yt_list[-1][y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]].astype(np.int64)\n",
    "        Y_GPU = paddle.to_tensor(y_cpu)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "        logits, feature = model.forward(*mydata)\n",
    "        \n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pred = paddle.argmax(probs,axis=-1)\n",
    "        logits_other = logits - logits * y_onehot\n",
    "        Pred_other = paddle.argmax(logits_other,axis=-1)\n",
    "\n",
    "        Py_temp[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]] = Py.numpy()\n",
    "        Pred_temp[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]] = Pred.numpy()  \n",
    "        \n",
    "        Probs_temp[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]] = probs.numpy()\n",
    "        Logits_temp[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]] = logits.numpy()\n",
    "  \n",
    "\n",
    "        feature_temp[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]] = feature.numpy()\n",
    "\n",
    "        if epoch_id < t_w:\n",
    "            loss = cold_loss_fn(logits,Y_GPU)\n",
    "        else:         \n",
    "            Y_GPU = paddle.where(paddle.to_tensor(OOD_mask[y_start[batch_id * batch_size] : y_start[min((batch_id+1)*batch_size, len(y_start)-1)]]), Pred_other, Y_GPU)\n",
    "            loss = cold_loss_fn(logits,Y_GPU)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "\n",
    "        if batch_id%int(3000/batch_size)==0:\n",
    "            pred_y_temp = np.argmax(logits.numpy(),axis=-1)\n",
    "            correct_n = np.sum(np.logical_and(y_cpu == pred_y_temp, y_cpu!=0))\n",
    "            pred_n = np.sum(pred_y_temp!=0)\n",
    "            total_n = np.sum(y_cpu!=0)\n",
    "            \n",
    "            precision = correct_n / total_n\n",
    "            recall = correct_n / pred_n\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            _loss = loss.numpy()\n",
    "            time_end=time.time()\n",
    "            print('batch_id:%4d, train loss :%.4f, f1:%.4f, recall:%.4f, precision:%.4f, time elapsed %4d'\n",
    "            %(batch_id, _loss, f1, recall, precision, time_end - time_start), flush=True)\n",
    "    print('epoch %d train complete'%epoch_id)\n",
    "\n",
    "    sch.step()\n",
    "\n",
    "    Py_list.append(Py_temp)\n",
    "\n",
    "\n",
    "    if epoch_id >= t_w:\n",
    "        nC_points = []\n",
    "        Py_mean = np.zeros((y_start[-1],))\n",
    "        for j in range(len(Py_list)):\n",
    "            Py_mean+=Py_list[j]\n",
    "        Py_mean/=len(Py_list)   \n",
    "        for j in range(n_class):\n",
    "            class_mask = Yt_list[0] == j\n",
    "            c_n = class_mask.sum()\n",
    "            c_th = np.sort(Py_mean[class_mask])[-int(c_n * n_C)]\n",
    "            nC_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "        nC_points = np.concatenate(nC_points)\n",
    "        L_batch = 1000\n",
    "        Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "        zC = paddle.to_tensor(feature_temp[nC_points])\n",
    "        fC = paddle.to_tensor(Probs_temp[nC_points])\n",
    "        yC = paddle.to_tensor(Y_onehot[nC_points])\n",
    "        fCcyC = fC - yC\n",
    "        lr = 1e-6\n",
    "        learning_risk = np.zeros((y_start[-1],)) \n",
    "        for j in range(int(np.ceil(y_start[-1]/L_batch))):\n",
    "            i_ind = np.arange(j*L_batch, min(y_start[-1],(j+1)*L_batch))\n",
    "            zi = paddle.to_tensor(feature_temp[i_ind])\n",
    "            fi = paddle.to_tensor(Probs_temp[i_ind])\n",
    "            yi = paddle.to_tensor(Y_onehot[i_ind])\n",
    "            zixzC = zi @ zC.transpose([1,0])\n",
    "            part_1_1 = (zixzC + 1) @ fCcyC\n",
    "            part1 = (part_1_1 * (yi-fi)).sum(axis=-1,keepdim=True)*4*lr/len(nC_points)\n",
    "            part1_all = part_1_1 * (fi-paddle.ones_like(yi))*4*lr/len(nC_points)\n",
    "\n",
    "            learning_risk[i_ind] = part1.numpy().ravel()\n",
    "\n",
    "        select = np.zeros((y_start[-1],),dtype=np.bool8)\n",
    "        for j in range(n_class):\n",
    "            class_mask = Yt_list[-1] == j\n",
    "            c_n = class_mask.sum()\n",
    "            if c_n > 2:\n",
    "                c_th = np.sort(learning_risk[class_mask])[-min(int(np.ceil(c_n * n_V)),c_n)]\n",
    "                select[np.logical_and(learning_risk>=c_th, class_mask)] = True\n",
    "        \n",
    "        temp_Yt_noised=np.where(select.ravel(), Pred_temp.ravel(), Yt_list[-1].ravel()).astype(int)\n",
    "    else:\n",
    "        temp_Yt_noised = Yt_list[0]\n",
    "        select = np.zeros((y_start[-1],))\n",
    "        Yt_list.append(temp_Yt_noised)\n",
    "    Yt_list.append(temp_Yt_noised)\n",
    "    print('epoch %d train cleaned, %d samples selected, %d neg samples selected'%\n",
    "            (epoch_id,np.sum(select),np.sum(np.logical_and(select,is_neg))))\n",
    "    print('total changed %d'%(np.sum(Yt_list[-1].ravel()!= train_annotated_y.ravel())))\n",
    "    return _loss, f1, recall, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T14:11:40.742322Z",
     "iopub.status.busy": "2023-04-25T14:11:40.741986Z",
     "iopub.status.idle": "2023-04-25T14:11:40.747590Z",
     "shell.execute_reply": "2023-04-25T14:11:40.746767Z",
     "shell.execute_reply.started": "2023-04-25T14:11:40.742295Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Py_list = []\n",
    "Yt_list = [train_annotated_y]\n",
    "\n",
    "acc_list = []\n",
    "f1_val_list = []\n",
    "loss_val_list = []\n",
    "loss_list = []\n",
    "\n",
    "loss_train_list = []\n",
    "f1_train_list = []\n",
    "recall_train_list = []\n",
    "precision_train_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "\n",
    "for epoch_id in range(epoch_n):\n",
    "    train_dis_f = open('./work/train_annotated_data.pkl','rb')\n",
    "    train_dis_loader = load_data_new(train_dis_f,)        \n",
    "    \n",
    "    _loss, f1, recall, precision = \\\n",
    "    train_predict_model(train_dis_loader,Yt_list,train_annotated_y_start,re_network1,acc_list,)\n",
    "    loss_train_list.append(_loss)\n",
    "    f1_train_list.append(f1)\n",
    "    recall_train_list.append(recall)\n",
    "    precision_train_list.append(precision)\n",
    "    loss_val,f1_val = validation_new(re_network1)\n",
    "    f1_val_list.append(f1_val)\n",
    "    loss_val_list.append(loss_val)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
