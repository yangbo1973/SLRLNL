{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:53.351591Z",
     "iopub.status.busy": "2023-05-29T06:44:53.351196Z",
     "iopub.status.idle": "2023-05-29T06:44:55.895135Z",
     "shell.execute_reply": "2023-05-29T06:44:55.894256Z",
     "shell.execute_reply.started": "2023-05-29T06:44:53.351557Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import MutableMapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Iterable, Mapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Sized\r\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.static import InputSpec\n",
    "from paddle.fluid.framework import core\n",
    "from paddle.vision import image_load\n",
    "\n",
    "from models.vgg import VGG, make_layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import functools\n",
    "from collections import Counter\n",
    "\n",
    "import gc\n",
    "from itertools import repeat\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "# !unzip -q data/data99524/raw_image.zip -d data/data99524/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:55.897773Z",
     "iopub.status.busy": "2023-05-29T06:44:55.897236Z",
     "iopub.status.idle": "2023-05-29T06:44:55.901977Z",
     "shell.execute_reply": "2023-05-29T06:44:55.901266Z",
     "shell.execute_reply.started": "2023-05-29T06:44:55.897744Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 128\n",
    "samples = 50000\n",
    "test_samples = 5000\n",
    "examples = samples\n",
    "n_class = 10\n",
    "training_epochs = 180\n",
    "\n",
    "image_size = 112\n",
    "\n",
    "\n",
    "t_w = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:55.903174Z",
     "iopub.status.busy": "2023-05-29T06:44:55.902857Z",
     "iopub.status.idle": "2023-05-29T06:44:56.010261Z",
     "shell.execute_reply": "2023-05-29T06:44:56.009481Z",
     "shell.execute_reply.started": "2023-05-29T06:44:55.903150Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(paddle.io.Dataset):\n",
    "    def __init__(self, num_samples, dir_, save_path, transform):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.dir = dir_\n",
    "        for root, dirs, files in os.walk(dir_):\n",
    "            pass\n",
    "        self.files = files\n",
    "        self.labels = np.array([int(f.split('_')[0]) for f in files])\n",
    "        np.save(save_path, self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_dir = os.path.join(self.dir,self.files[index])\n",
    "        image = image_load(data_dir)\n",
    "        data = np.array(image)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return index, data\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:56.011792Z",
     "iopub.status.busy": "2023-05-29T06:44:56.011273Z",
     "iopub.status.idle": "2023-05-29T06:44:56.133832Z",
     "shell.execute_reply": "2023-05-29T06:44:56.133047Z",
     "shell.execute_reply.started": "2023-05-29T06:44:56.011765Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\n",
    "\n",
    "\n",
    "mean,std = ([131.0127508203125, 123.49685918457031, 107.42944918945312],[69.10535081507089, 67.92458442633819,70.61935597148465])\n",
    "mean = np.array(mean).reshape(1,1,3)\n",
    "std = np.array(std).reshape(1,1,3)\n",
    "\n",
    "transform_robust = T.Compose([\n",
    "                    T.RandomHorizontalFlip(0.5),\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "transform_clean = T.Compose([\n",
    "                    # T.ColorJitter(0.125,0.4,0.4,0.08),\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std),\n",
    "                    # T.Transpose(order=(2,1,0,)),\n",
    "                    ])\n",
    "\n",
    "train_dataset = Dataset(samples, 'data/data99524/training', 'data/data99524/train_Y.npy', transform_robust)\n",
    "test_dataset = Dataset(test_samples, 'data/data99524/testing', 'data/data99524/test_Y.npy', transform_clean)\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6,)\n",
    "valid_loader = paddle.io.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6,)\n",
    "train_Y = np.load('data/data99524/train_Y.npy')\n",
    "test_Y = np.load('data/data99524/test_Y.npy')\n",
    "\n",
    "train_Y_noised = train_Y\n",
    "Yt_list = [train_Y_noised]\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:56.135495Z",
     "iopub.status.busy": "2023-05-29T06:44:56.134952Z",
     "iopub.status.idle": "2023-05-29T06:44:59.553301Z",
     "shell.execute_reply": "2023-05-29T06:44:59.552394Z",
     "shell.execute_reply.started": "2023-05-29T06:44:56.135466Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:44:56.144913  2397 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0529 14:44:56.149894  2397 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n"
     ]
    }
   ],
   "source": [
    "class Orthogonal_loss(paddle.nn.Layer):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.shape[0]\n",
    "        m = x.shape[1]\n",
    "\n",
    "        I = paddle.eye(m)\n",
    "        e = x - x.mean(axis=0, keepdim = True)\n",
    "        m_nonz = (e.sum(axis = 0) != 0).sum()\n",
    "        \n",
    "        cov = e.t() @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        \n",
    "        select_i = paddle.argmax(cov2 - cov2 * I, axis = 1)\n",
    "        cov_m = (paddle.nn.functional.one_hot(select_i, m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / (m_nonz*n)\n",
    "        return result\n",
    "\n",
    "vgg19_cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512,\n",
    "        'M', 512, 512, 512, 512, 'M']\n",
    "\n",
    "network_3 = VGG(make_layers(vgg19_cfg, batch_norm = True))\n",
    "network_3.set_state_dict(paddle.load(r'data/data184212/vgg19_bn_pretrain.pdparams'))\n",
    "feature_num = network_3.classifier[6].parameters()[0].shape[0]\n",
    "\n",
    "\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "loss_ortho = Orthogonal_loss()\n",
    "scheduel_e = paddle.optimizer.lr.ExponentialDecay(learning_rate = learning_rate, gamma = 0.93)\n",
    "\n",
    "opt = paddle.optimizer.Momentum(learning_rate=scheduel_e,\n",
    "parameters=network_3.parameters(),weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:59.554829Z",
     "iopub.status.busy": "2023-05-29T06:44:59.554442Z",
     "iopub.status.idle": "2023-05-29T06:44:59.567804Z",
     "shell.execute_reply": "2023-05-29T06:44:59.567156Z",
     "shell.execute_reply.started": "2023-05-29T06:44:59.554802Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    loss_eval = 0\n",
    "    acc_eval = 0\n",
    "    feature_num = network_3.classifier[6].parameters()[0].shape[0]\n",
    "    network_3.eval()\n",
    "    PP_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Py_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Pm_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Feature_temp = np.zeros((test_samples,feature_num),dtype = np.float32)\n",
    "    with paddle.no_grad():\n",
    "        for batch_id, (ind,X_data) in enumerate(valid_loader()):\n",
    "            ind = np.array(ind)\n",
    "            Y_data = np.array(test_Y[ind]).astype(np.int64)\n",
    "            temp_X = paddle.to_tensor(X_data)\n",
    "            Y_GPU = paddle.to_tensor(Y_data)\n",
    "            y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "            classifier = network_3.classifier\n",
    "            first = network_3.avgpool(network_3.features(temp_X))\n",
    "            first = paddle.reshape(first, (first.shape[0], -1))\n",
    "            feature = classifier[5](classifier[4](classifier[3](classifier[2](classifier[1](classifier[0](first))))))\n",
    "\n",
    "            logits = network_3.classifier[6](feature)[:,:n_class]\n",
    "            probs = paddle.nn.functional.softmax(logits)\n",
    "            PP = paddle.sum(probs * probs, axis = -1)\n",
    "            Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "            Pm = paddle.max(probs, axis = -1)\n",
    "            PP_temp[ind] = PP.numpy()\n",
    "            Py_temp[ind] = Py.numpy()\n",
    "            Pm_temp[ind] = Pm.numpy()\n",
    "            Feature_temp[ind] = feature.numpy()\n",
    "            try:\n",
    "                loss = loss_fn(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "            except:\n",
    "                loss = loss_fn(logits, y_onehot)\n",
    "            acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "            loss_eval += loss.numpy()\n",
    "            acc_eval += acc.numpy()\n",
    "\n",
    "        True_PP = np.mean(PP_temp[Py_temp >= Pm_temp])\n",
    "        False_PP = np.mean(PP_temp[Py_temp < Pm_temp])\n",
    "        loss_eval/=(batch_id+1)\n",
    "        acc_eval/=(batch_id+1)\n",
    "        lossb = relevant_hard_np(Feature_temp)\n",
    "    return loss_eval, acc_eval, True_PP, False_PP, lossb, Feature_temp\n",
    "\n",
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    r = np.corrcoef(x.T)\n",
    "    r[np.isnan(r)] = 0\n",
    "    r = r ** 2\n",
    "    \n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T06:44:59.569163Z",
     "iopub.status.busy": "2023-05-29T06:44:59.568688Z",
     "iopub.status.idle": "2023-05-29T06:44:59.573626Z",
     "shell.execute_reply": "2023-05-29T06:44:59.573003Z",
     "shell.execute_reply.started": "2023-05-29T06:44:59.569138Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs_list = []\n",
    "Py_list = []\n",
    "PP_list = []\n",
    "Pm_list = []\n",
    "UU_list = []\n",
    "Pred_list = []\n",
    "acc_r_list = []\n",
    "error_list = []\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "TPP_list = []\n",
    "FPP_list = []\n",
    "Tacc_list = []\n",
    "Tacco_list = []\n",
    "warnings.filterwarnings(\"ignore\", category=Warning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_list = []\n",
    "logits_other_list = []\n",
    "Pred_list = []\n",
    "Py_list = []\n",
    "PP_list = []\n",
    "Pm_list = []\n",
    "Pm_other_list = []\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "False_PP_list = []\n",
    "True_PP_list = []\n",
    "du_p_list = []\n",
    "du_y_list = []\n",
    "du_prob_list = []\n",
    "\n",
    "logits_list = []\n",
    "l3_feature_list = []\n",
    "l4_feature_list = []\n",
    "feature_list = []\n",
    "feature_pred_list = []\n",
    "noise_n_list = []\n",
    "select_n_list = []\n",
    "\n",
    "delta_L_list = []\n",
    "anchor_score_list = []\n",
    "delta_L_all_list = []\n",
    "part_1_list = []\n",
    "part_2_list = []\n",
    "part_t_list = []\n",
    "\n",
    "\n",
    "lossb_list = []\n",
    "score_list = [np.random.rand(samples,)]\n",
    "\n",
    "Pred_onehot_temp = np.ones((samples,n_class),dtype=np.float32)\n",
    "\n",
    "softmax = paddle.nn.Softmax(axis=0)\n",
    "\n",
    "for epoch_id in range(training_epochs):\n",
    "    network_3.train()\n",
    "    feature_num = network_3.classifier[6].parameters()[0].shape[0]\n",
    "    loader = train_loader()\n",
    "\n",
    "    loss_train = 0\n",
    "    losso_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    PP_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pm_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pm_other_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp_other = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp_other = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "    mask_temp = np.zeros((samples,), dtype = np.bool)\n",
    "    \n",
    "    weight_temp = np.zeros((samples,),dtype=np.float32)\n",
    " \n",
    "    if len(Py_list)>0:\n",
    "        relabel_score1=Py_list[-1] - PP_list[-1]\n",
    "        relabel_score2=Py_list[-1] - Pm_other_list[-1]\n",
    "        PP_mean = PP_list[-1]\n",
    "        Py_mean = Py_list[-1]\n",
    "    else:\n",
    "        relabel_score1 = np.zeros((samples,))\n",
    "        relabel_score2 = np.zeros((samples,))\n",
    "        PP_mean = np.zeros((samples,))\n",
    "        Py_mean = np.zeros((samples,))\n",
    "\n",
    "    thres = np.sort(score_list[-1])[int(samples*0.8)]\n",
    "    r_mask = score_list[-1] >= thres\n",
    "    \n",
    "    side_mask_rand = np.logical_and(r_mask,np.random.rand(samples,) < 0.2)\n",
    "\n",
    "    if len(Py_list) > 1:\n",
    "        score = Py_list[-1]\n",
    "    else:\n",
    "        score = np.random.rand(samples,)\n",
    "    OOD_mask = score < np.sort(score)[int(samples * 0.01)]\n",
    "\n",
    "    Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "    Y_onehot_0 = np.eye(n_class)[Yt_list[0]]\n",
    "    \n",
    "    for batch_id, (ind, X_data) in tqdm.tqdm(enumerate(loader)):\n",
    "        ind = np.array(ind)\n",
    "\n",
    "        Y_data = np.array(Yt_list[-1][ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[ind]).astype(np.int64)\n",
    "        pred_onehot_temp = Pred_onehot_temp[ind]\n",
    "        temp_X = paddle.to_tensor(X_data)\n",
    "        Y_GPU = paddle.to_tensor(Y_data)\n",
    "        Y_GPU_ori = paddle.to_tensor(Y_data_ori)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "        classifier = network_3.classifier\n",
    "        first = network_3.avgpool(network_3.features(temp_X))\n",
    "        first = paddle.reshape(first, (first.shape[0], -1))\n",
    "        feature = classifier[5](classifier[4](classifier[3](classifier[2](classifier[1](classifier[0](first))))))\n",
    "\n",
    "        logits = network_3.classifier[6](feature)[:,:n_class]\n",
    "\n",
    "\n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        PP = paddle.sum(probs * probs, axis = -1)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pm = paddle.max(probs, axis = -1)\n",
    "        Pm_other = paddle.max(probs - probs * y_onehot, axis = -1)\n",
    "        Pred = paddle.argmax(probs,axis=-1)\n",
    "        logits_other = logits - y_onehot*1e10\n",
    "        Pred_other = paddle.argmax(logits_other,axis=-1)\n",
    "\n",
    "        alpha = 1\n",
    "        loss_o = loss_ortho(feature)\n",
    "        if epoch_id < t_w:\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "            \n",
    "        else:\n",
    "            boundary_mask_ = paddle.to_tensor(OOD_mask[ind])\n",
    "            Y_GPU = paddle.where(boundary_mask_, Pred_other, Y_GPU)\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "            \n",
    "        PP_temp[ind] = PP.numpy()\n",
    "        Py_temp[ind] = Py.numpy()\n",
    "        Pm_temp[ind] = Pm.numpy()\n",
    "        Pm_other_temp[ind] = Pm_other.numpy()\n",
    "        Pred_temp[ind] = Pred.numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.numpy()\n",
    "        Logits_temp[ind] = logits.numpy()\n",
    "        Logits_temp_other[ind] = logits_other.numpy()\n",
    "        Pred_temp_other[ind] = Pred_other.numpy()\n",
    "        feature_temp[ind] = feature.numpy()\n",
    "                \n",
    "        \n",
    "        acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "        acc_ori = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "        loss_ori = loss_fn(logits,paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "\n",
    "        loss_train += loss.numpy()\n",
    "        losso_train += loss_o.numpy()\n",
    "        acc_train += acc.numpy()\n",
    "        acc_train_ori += acc_ori.numpy()\n",
    "        loss_train_ori += loss_ori.numpy()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        \n",
    "    scheduel_e.step()\n",
    "    loss_train/=(batch_id+1)\n",
    "    losso_train/=(batch_id+1)\n",
    "    acc_train/=(batch_id+1)\n",
    "    acc_train_ori/=(batch_id+1)\n",
    "    loss_train_ori/=(batch_id+1)\n",
    "\n",
    "    print('epoch %d train complete'%epoch_id)\n",
    "    loss_eval, acc_eval, True_PP, False_PP, lossb_eval, feature_val = validation()\n",
    "\n",
    "    loss_list.append(loss_eval)\n",
    "    acc_list.append(acc_eval)\n",
    "    True_PP_list.append(True_PP)\n",
    "    False_PP_list.append(False_PP)\n",
    "    Pred_list.append(Pred_temp)\n",
    "    PP_list.append(PP_temp)\n",
    "    Py_list.append(Py_temp)\n",
    "    Pm_list.append(Pm_temp)\n",
    "    Pm_other_list.append(Pm_other_temp)\n",
    "\n",
    "    \n",
    "    lossb_list.append(lossb_eval)\n",
    "\n",
    "    logits_list.append(Logits_temp)\n",
    "    logits_other_list.append(Logits_temp_other)\n",
    "    if np.mod(epoch_id,20) == 0:\n",
    "        feature_list.append(feature_temp)\n",
    "    else:\n",
    "        feature_list.append([])\n",
    "    \n",
    "    print('train loss:%.4f,train losso:%.4f, train acc:%.4f, train acc ori:%.4f,  eval loss:%.4f, eval acc:%.4f, lossb eval:%e'\n",
    "    %(loss_train,losso_train, acc_train, acc_train_ori, loss_eval, acc_eval, lossb_eval))  \n",
    "\n",
    "    anchor_points = []\n",
    "    anchor_mask = np.zeros((samples,),dtype=bool)\n",
    "    Py_mean = np.zeros((samples,))\n",
    "    \n",
    "    for j in range(len(Py_list)):\n",
    "        Py_mean+=Py_list[j]\n",
    "    Py_mean/=len(Py_list)   \n",
    "\n",
    "    for j in range(n_class):\n",
    "        class_mask = Yt_list[0] == j\n",
    "        c_n = class_mask.sum()\n",
    "        c_th = np.sort(Py_mean[class_mask])[-int(c_n * 0.10)]\n",
    "        anchor_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "    anchor_points = np.concatenate(anchor_points)\n",
    "    anchor_mask[anchor_points] = True\n",
    "    \n",
    "    L_batch = 1000\n",
    "    Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "    zC = paddle.to_tensor(feature_temp[anchor_points])\n",
    "    fC = paddle.to_tensor(Probs_temp[anchor_points])\n",
    "    yC = paddle.to_tensor(Y_onehot[anchor_points])\n",
    "    fCcyC = fC - yC\n",
    "    # lr = opt.get_lr()/10\n",
    "    lr = 1e-6\n",
    "    delta_L = np.zeros((samples,))\n",
    "    part1_L = np.zeros((samples,)) \n",
    "    part2_L = np.zeros((samples,)) \n",
    "    anchor_score = np.zeros((samples,))\n",
    "    delta_L_all = np.zeros((samples, n_class))\n",
    "    for j in range(int(np.ceil(samples/L_batch))):\n",
    "        i_ind = np.arange(j*L_batch, min(samples,(j+1)*L_batch))\n",
    "        zi = paddle.to_tensor(feature_temp[i_ind])\n",
    "        fi = paddle.to_tensor(Probs_temp[i_ind])\n",
    "        yi = paddle.to_tensor(Y_onehot[i_ind])\n",
    "        zixzC = zi @ zC.transpose([1,0])\n",
    "        part_1_1 = (zixzC + 1) @ fCcyC\n",
    "        part1 = (part_1_1 * (yi-fi)).sum(axis=-1,keepdim=True)*4*lr/len(anchor_points)\n",
    "        part1_all = part_1_1 * (fi-paddle.ones_like(yi))*4*lr/len(anchor_points)\n",
    "\n",
    "        part1_L[i_ind] = part1.numpy().ravel()\n",
    "\n",
    "    select = np.zeros((samples,),dtype=np.bool8)\n",
    "    for j in range(n_class):\n",
    "        class_mask = Yt_list[-1] == j\n",
    "        c_n = class_mask.sum()\n",
    "        c_th = np.sort(part1_L[class_mask])[-min(int(c_n * (0.01)),c_n)]\n",
    "        select[np.logical_and(part1_L>=c_th, class_mask)] = True\n",
    "    if epoch_id < t_w:\n",
    "        temp_Yt_noised = Yt_list[-1]\n",
    "    else:\n",
    "        temp_Yt_noised=np.where(select.ravel(), Pred_temp.ravel(), Yt_list[-1].ravel()).astype(int)\n",
    "\n",
    "    is_noise = temp_Yt_noised != train_Y\n",
    "    max_noised_class = -999\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = train_Y == j_\n",
    "        noise_n = np.logical_and(class_mask, is_noise).sum()\n",
    "        if noise_n > max_noised_class:\n",
    "            max_noised_class = noise_n\n",
    "    Yt_list.append(temp_Yt_noised)\n",
    "    print('epoch %d train cleaned, %d samples selected, %d samples changed'%(\n",
    "        epoch_id,np.sum(Yt_list[-1]!=Yt_list[-2]), np.sum(Yt_list[-1]!=Yt_list[0])))\n",
    "    Yt_remain_noise = np.sum(is_noise)\n",
    "    print('total remain noise:%.4d, max class noise:%d'%(Yt_remain_noise, max_noised_class))\n",
    "    noise_n_list.append(Yt_remain_noise)\n",
    "    select_n_list.append(np.sum(select))\n",
    "    score_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-29T07:02:00.435384Z",
     "iopub.status.idle": "2023-05-29T07:02:00.435809Z",
     "shell.execute_reply": "2023-05-29T07:02:00.435650Z",
     "shell.execute_reply.started": "2023-05-29T07:02:00.435630Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(max(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
